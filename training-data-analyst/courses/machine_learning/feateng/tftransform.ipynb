{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Exploring tf.transform </h1>\n",
    "\n",
    "tf.transform is in beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "pip install --upgrade tensorflow_transform\n",
    "pip install --upgrade protobuf==3.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import google.cloud.ml as ml\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import shutil\n",
    "print tf.__version__\n",
    "print ml.sdk_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = 'cloud-training-demos'    # CHANGE THIS\n",
    "BUCKET = 'cloud-training-demos-ml'  # CHANGE THIS\n",
    "REGION = 'us-central1' # CHANGE THIS\n",
    "\n",
    "os.environ['PROJECT'] = PROJECT # for bash\n",
    "os.environ['BUCKET'] = BUCKET # for bash\n",
    "os.environ['REGION'] = REGION # for bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow_transform import coders\n",
    "from tensorflow_transform.beam import impl as tft\n",
    "from tensorflow_transform.beam import io\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import dataset_schema\n",
    "\n",
    "from tensorflow_transform import api\n",
    "from tensorflow_transform import mappers\n",
    "\n",
    "CSV_COLUMNS = 'fare_amount,dayofweek,hourofday,pickuplon,pickuplat,dropofflon,dropofflat,passengers,key'.split(',')\n",
    "SCALE_COLUMNS = ['pickuplon','pickuplat','dropofflon','dropofflat','passengers']\n",
    "LABEL_COLUMN = 'fare_amount'\n",
    "KEY_FEATURE_COLUMN = 'key'\n",
    "\n",
    "class PathConstants:\n",
    "  def __init__(self):\n",
    "    self.TEMP_DIR = 'tmp'\n",
    "    self.TRANSFORM_FN_DIR = 'transform_fn'\n",
    "    self.RAW_METADATA_DIR = 'raw_metadata'\n",
    "    self.TRANSFORMED_METADATA_DIR = 'transformed_metadata'\n",
    "    self.TRANSFORMED_TRAIN_DATA_FILE_PREFIX = 'features_train'\n",
    "    self.TRANSFORMED_EVAL_DATA_FILE_PREFIX = 'features_eval'\n",
    "    self.TRANSFORMED_PREDICT_DATA_FILE_PREFIX = 'features_predict'\n",
    "    self.TRAIN_RESULTS_FILE = 'train_results'\n",
    "    self.DEPLOY_SAVED_MODEL_DIR = 'saved_model'\n",
    "    self.MODEL_EVALUATIONS_FILE = 'model_evaluations'\n",
    "    self.BATCH_PREDICTION_RESULTS_FILE = 'batch_prediction_results'\n",
    "    \n",
    "def make_preprocessing_fn():\n",
    "  # stop-gap ...\n",
    "  def _scalar_to_vector(scalar):\n",
    "    # FeatureColumns expect shape (batch_size, 1), not just (batch_size)\n",
    "    return api.map(lambda x: tf.expand_dims(x, -1), scalar)\n",
    "  \n",
    "  def preprocessing_fn(inputs):\n",
    "    result = {col: _scalar_to_vector(inputs[col]) for col in CSV_COLUMNS}\n",
    "    for name in SCALE_COLUMNS:\n",
    "      result[name] = _scalar_to_vector(mappers.scale_to_0_1(inputs[name]))\n",
    "\n",
    "    # use tft.map() to create new columns\n",
    "    # tft.scale_to_0_1\n",
    "    # tft.map(tf.sparse_column_with_keys, inputs['gender'], Statistic({'M', 'F'})\n",
    "    # tft.string_to_int(inputs[name], frequency_threshold=frequency_threshold)\n",
    "    return result\n",
    "\n",
    "  return preprocessing_fn\n",
    "\n",
    "def make_input_schema(mode):\n",
    "  input_schema = {}\n",
    "  if mode != tf.contrib.learn.ModeKeys.INFER:\n",
    "      input_schema[LABEL_COLUMN] = tf.FixedLenFeature(shape=[], dtype=tf.float32, default_value=0.0)\n",
    "  for name in ['dayofweek', 'key']:\n",
    "      input_schema[name] = tf.FixedLenFeature(shape=[], dtype=tf.string, default_value='null')\n",
    "  for name in ['hourofday']:\n",
    "      input_schema[name] = tf.FixedLenFeature(shape=[], dtype=tf.int64, default_value=0)\n",
    "  for name in SCALE_COLUMNS:\n",
    "      input_schema[name] = tf.FixedLenFeature(shape=[], dtype=tf.float32, default_value=0.0)\n",
    "      \n",
    "  input_schema = dataset_schema.from_feature_spec(input_schema)\n",
    "  return input_schema\n",
    "\n",
    "def make_coder(schema, mode):\n",
    "  import copy\n",
    "  column_names = copy.deepcopy(CSV_COLUMNS)\n",
    "  if mode == tf.contrib.learn.ModeKeys.INFER:\n",
    "    column_names.pop(LABEL_COLUMN)\n",
    "  coder = coders.CsvCoder(column_names, schema)\n",
    "  return coder\n",
    "\n",
    "def preprocess_all(pipeline, training_data, eval_data, predict_data, output_dir, mode=tf.contrib.learn.ModeKeys.TRAIN):\n",
    "  path_constants = PathConstants()\n",
    "  work_dir = os.path.join(output_dir, path_constants.TEMP_DIR)\n",
    "  \n",
    "  # create schema\n",
    "  input_schema = make_input_schema(mode)\n",
    "\n",
    "  # coder\n",
    "  coder = make_coder(input_schema, mode)\n",
    "\n",
    "  # 3) Read from text using the coder.\n",
    "  train_data = (\n",
    "      pipeline\n",
    "      | 'ReadTrainingData' >> beam.io.ReadFromText(training_data)\n",
    "      | 'ParseTrainingCsv' >> beam.Map(coder.decode))\n",
    "\n",
    "  evaluate_data = (\n",
    "      pipeline\n",
    "      | 'ReadEvalData' >> beam.io.ReadFromText(eval_data)\n",
    "      | 'ParseEvalCsv' >> beam.Map(coder.decode))\n",
    "\n",
    "  # metadata\n",
    "  input_metadata = dataset_metadata.DatasetMetadata(schema=input_schema)\n",
    "\n",
    "  _ = (input_metadata\n",
    "       | 'WriteInputMetadata' >> io.WriteMetadata(\n",
    "           os.path.join(output_dir, path_constants.RAW_METADATA_DIR),\n",
    "           pipeline=pipeline))\n",
    "\n",
    "  preprocessing_fn = make_preprocessing_fn()\n",
    "  (train_dataset, train_metadata), transform_fn = (\n",
    "      (train_data, input_metadata)\n",
    "      | 'AnalyzeAndTransform' >> tft.AnalyzeAndTransformDataset(\n",
    "          preprocessing_fn, work_dir))\n",
    "\n",
    "  # WriteTransformFn writes transform_fn and metadata to fixed subdirectories\n",
    "  # of output_dir, which are given by path_constants.TRANSFORM_FN_DIR and\n",
    "  # path_constants.TRANSFORMED_METADATA_DIR.\n",
    "  transform_fn_is_written = (transform_fn | io.WriteTransformFn(output_dir))\n",
    "\n",
    "  # TODO(b/34231369) Remember to eventually also save the statistics.\n",
    "\n",
    "  (evaluate_dataset, evaluate_metadata) = (\n",
    "      ((evaluate_data, input_metadata), transform_fn)\n",
    "      | 'TransformEval' >> tft.TransformDataset())\n",
    "\n",
    "  train_coder = coders.ExampleProtoCoder(train_metadata.schema)\n",
    "  _ = (train_dataset\n",
    "       | 'SerializeTrainExamples' >> beam.Map(train_coder.encode)\n",
    "       | 'WriteTraining'\n",
    "       >> beam.io.WriteToTFRecord(\n",
    "           os.path.join(output_dir,\n",
    "                        path_constants.TRANSFORMED_TRAIN_DATA_FILE_PREFIX),\n",
    "           file_name_suffix='.tfrecord.gz'))\n",
    "\n",
    "  evaluate_coder = coders.ExampleProtoCoder(evaluate_metadata.schema)\n",
    "  _ = (evaluate_dataset\n",
    "       | 'SerializeEvalExamples' >> beam.Map(evaluate_coder.encode)\n",
    "       | 'WriteEval'\n",
    "       >> beam.io.WriteToTFRecord(\n",
    "           os.path.join(output_dir,\n",
    "                        path_constants.TRANSFORMED_EVAL_DATA_FILE_PREFIX),\n",
    "           file_name_suffix='.tfrecord.gz'))\n",
    "\n",
    "  if predict_data:\n",
    "    predict_mode = tf.contrib.learn.ModeKeys.INFER\n",
    "    predict_schema = make_input_schema(mode=predict_mode)\n",
    "    tsv_coder = make_coder(predict_schema, mode=predict_mode)\n",
    "    predict_coder = coders.ExampleProtoCoder(predict_schema)\n",
    "    _ = (pipeline\n",
    "         | 'ReadPredictData' >> beam.io.ReadFromText(predict_data,\n",
    "                                                     coder=tsv_coder)\n",
    "         # TODO(b/35194257) Obviate the need for this explicit serialization.\n",
    "         | 'EncodePredictData' >> beam.Map(predict_coder.encode)\n",
    "         | 'WritePredictData' >> beam.io.WriteToTFRecord(\n",
    "             os.path.join(output_dir,\n",
    "                          path_constants.TRANSFORMED_PREDICT_DATA_FILE_PREFIX),\n",
    "             file_name_suffix='.tfrecord.gz'))\n",
    "\n",
    "  # Workaround b/35366670, to ensure that training and eval don't start before\n",
    "  # the transform_fn is written.\n",
    "  train_dataset |= beam.Map(\n",
    "      lambda x, y: x, y=beam.pvalue.AsSingleton(transform_fn_is_written))\n",
    "  evaluate_dataset |= beam.Map(\n",
    "      lambda x, y: x, y=beam.pvalue.AsSingleton(transform_fn_is_written))\n",
    "\n",
    "  return transform_fn, train_dataset, evaluate_dataset\n",
    "\n",
    "train_data_paths='./sample/train.csv' \n",
    "eval_data_paths='./sample/valid.csv'  \n",
    "output_dir='./taxi_preproc' \n",
    "predict_data_paths=None\n",
    "\n",
    "shutil.rmtree('./taxi_preproc', ignore_errors=True)\n",
    "p = beam.Pipeline()\n",
    "transform_fn, train_dataset, eval_dataset = preprocess_all(\n",
    "      p, train_data_paths, eval_data_paths, predict_data_paths, output_dir)\n",
    "\n",
    "p.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls taxi_preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Train off preprocessed data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "rm -rf taxifare.tar.gz taxi_trained\n",
    "export PYTHONPATH=${PYTHONPATH}:/content/training-data-analyst/courses/machine_learning/feateng/taxifare\n",
    "python -m trainer.task \\\n",
    "   --train_data_paths=\"/content/training-data-analyst/courses/machine_learning/feateng/taxi_preproc/features_train-00001*\" \\\n",
    "   --eval_data_paths=\"/content/training-data-analyst/courses/machine_learning/feateng/taxi_preproc/features_eval-00001*\"  \\\n",
    "   --output_dir=/content/training-data-analyst/courses/machine_learning/feateng/taxi_trained \\\n",
    "   --num_epochs=10 --job-dir=/tmp \\\n",
    "   --format=tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls /content/training-data-analyst/courses/machine_learning/feateng/taxi_trained/export/Servo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%writefile /tmp/test.json\n",
    "{\"dayofweek\":\"Thu\",\"hourofday\":17,\"pickuplon\": -73.885262,\"pickuplat\": 40.773008,\"dropofflon\": -73.987232,\"dropofflat\": 40.732403,\"passengers\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "model_dir=$(ls /content/training-data-analyst/courses/machine_learning/feateng/taxi_trained/export/Servo/)\n",
    "gcloud ml-engine local predict \\\n",
    "    --model-dir=/content/training-data-analyst/courses/machine_learning/feateng/taxi_trained/export/Servo/${model_dir} \\\n",
    "    --json-instances=/tmp/test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2016 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
